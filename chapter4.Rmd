# Clustering and Classification

Loading Boston data and reading its summary, structure and creating a pair plot for the understanding of the data.

```{r, include=FALSE}
library(MASS)
library(tidyverse)
library(corrplot)
# loading the data
data("Boston")
```


```{r, echo=TRUE}
#Structure of the dataset
str(Boston)

#Summary of the dataset
summary(Boston)

# plotting matrix of the variables
pairs(Boston)
```

Now finding the correlation between different variables of the loaded data. 

```{r, include=FALSE}

# calculate the correlation matrix and round it
cor_matrix<-  cor(Boston)%>% round(2) 

```

```{r}

# print the correlation matrix
cor_matrix
# visualizing the correlation matrix
corrplot(cor_matrix, method="circle")
```

Now scaling the dataset and converting it into data frame to perform comparisons and predictions on the dataset.

```{r, include=FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)
```

```{r}
# summaries of the scaled variables
summary(boston_scaled)
```

```{r, include=FALSE}
# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

AFter scaling the dataset, next task was to see the summary of the crime variable and then convert the crime variable into quantiles. 

```{r}

summary(boston_scaled$crim)

bins <- quantile(boston_scaled$crim)
bins
```

Then created a categorical variable and named it 'crime'. 

```{r, include=FALSE}
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
```

```{r}

# look at the table of the new factor crime
table(crime)
```

```{r, include=FALSE}
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Now as we have created a categorical variable crime and scaled our dataset, we can train our data and then run Linear Data Analysis for the prediction of crime. We have used 80, 20 percent of the data for training and testing respectively. 

```{r, include=FALSE}


# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)



# linear discriminant analysis
lda.fit <- lda(crime~., data = train)
```

```{r}

# print the result of LDA model
lda.fit
```

```{r, include=FALSE}


# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)
```

```{r}

# plot the lda results
plot(lda.fit, dimen = 2, col= classes, pch=classes)
lda.arrows(lda.fit, myscale = 2)
```

```{r, include=FALSE}

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
```

```{r}
# Comparing the results of correct classes and predicted classes, in tabular form.  
table(correct =correct_classes , predicted = lda.pred$class)

```

Our model shows pretty good results for the given data. The graph and the table are closely predicting the correct result.

Now as we have trained our model, we will use K-means clustering method and explore the variables and will find their relevant clusters.

```{r, include=FALSE}


# euclidean distance matrix
dist_eu <- dist(Boston)
# manhattan distance matrix
dist_man <- dist(Boston, method = "manhattan")
```

We used two methods in distance measuing, Euclidean and Manhattan. 
Following are the results of both methods.
```{r}

# summary of the Euclidean distances
summary(dist_eu)

# summary of the Manhattan distances
summary(dist_man)
```

```{r, include=FALSE}
# Boston dataset is available

# k-means clustering
km <-kmeans(Boston, centers = 3)
```

```{r}
# plot the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)
```

```{r, include=FALSE}

library(ggplot2)
# MASS, ggplot2 and Boston dataset are available
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
```

```{r}
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

```{r, include=FALSE}

# k-means clustering
km <-kmeans(Boston, centers = 2)
```
The thoery states that "When you plot the number of clusters and the total within cluster sum of squares (WCSS), the optimal number of clusters is when the total WCSS drops radically". Our result shows that when the number of clusters were two, WCSS drops exponentially. It means that in our case the optimal number of clusters are two. 

```{r}
# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

```

